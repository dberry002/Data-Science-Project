---
title: "Predicting Premier League Final Standings"
author: "By: Daniel and Aiden"
execute:
  echo: true
format:
  revealjs:
    theme: default 
editor: 
  markdown: 
    wrap: 72
engine: knitr
---

```{r packages}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(reticulate)
library(openintro)
library(readr)
library(ggcorrplot)
library(dplyr)
library(caret)      
library(Metrics)      
library(ggplot2)
library(broom)
library(knitr)

set.seed(1234)
```

# Research Question

**Can a team's performance metrics after the first half of the English
Premier League season reliably predict their final standings?**

::: center
![](StadiumImage.jpg){fig-align="center" width="275"}
:::

# Motivation

-   The EPL is one of the most competitive soccer leagues in the world.

-   Early-season performance often hints at final outcomes

-   Stronger predictions can help teams and analyst make strategic mid
    season decisions.

# Data Description

**Source:** worldfootball.net

**Collection:** Manual copy into excel, feature engineering, saved as
CSV.

**Scope:** 5 Premier League Seasons

**Outcome Variable:** Final_Points

**Explanatory Variables:** Goals_For, Goals_Against, Differential, Win
Percentage, and many more.

# Data Cleaning

1.  Removed irrelevant columns (e.g.,, logos).

2.  Parsed goal differential into separate Goals_For and Goals_Against
    columns.

3.  Created new variables: GPM, GAPM, ADR, Goals_By_Win.

4.  Categorized teams: Aggressive, Dominant, Defensive, and Under
    performers.

```{r}
#| echo: false
#| message: false
#| warning: false
# First for our slides it is important to import the data again and set it up the way it was in our analysis. Just made a copy of the data set to make it easier to load our models.
PL_Data <- read_csv("PL_data2.csv")

```

```{r Column Renaming}
#| echo: false
#| message: false
#| warning: false
# We want to make sure an audience that is not familiar with sports/soccer terms understand what are variables stand for. So renaming the columns is good practice to do this. 

PL_Data <- PL_Data %>%
  rename(Differential = Dif, Goals_For = GF, Goals_Against = GA, Goals_Per_Match = GPM, Goals_Against_Per_Match = GAPM, Attack_Defense_Ratio = ADR, Goals_Against_By_Wins = GCBW)

# Also hiding this code but talked about it in the presentation.
```

# Variable Correlations

```{r Correlation Matrix, fig.width=10, fig.height=10, warning = FALSE}
#| echo: false
#| message: false
#| warning: false
correlation_matrix <- cor(PL_Data %>% select(
  Final_Points, Points, W, L, D, Goals_For, Goals_Against,
  Differential, Goals_Per_Match, Goals_Against_Per_Match,
  Attack_Defense_Ratio, Goals_Against_By_Wins, Goals_by_Win
))

ggcorrplot(
  correlation_matrix,
  lab = TRUE,
  type = "lower",
  lab_size = 3.5,
  tl.cex = 10,
  tl.srt = 45,           # Rotate text
  colors = c("blue", "white", "red"),
  ggtheme = theme_minimal()
) +
  labs(
    title = "Heatmap of Midseason Metrics and Final Points",
    subtitle = "Red: strong positive correlation; Blue: strong negative correlation"
  ) +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_text(size = 10)
  )

```

# Relationship Between Variables

-   As GAPM increases—meaning teams are conceding more goals per
    match—their final point totals tend to decrease.

    ```{r}
    #| echo: false
    #| fig-align: center
    knitr::include_graphics("Linear.png")
    ```

```{r}
#| echo: false
#| message: false
#| warning: false
ggplot(PL_Data, aes(x = Goals_Against_Per_Match, y = Final_Points)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Points vs. Goals Against Per Match",
       x = "Goals Against Per Match (GPM)",
       y = "Points") +
  theme_minimal()
```

# Model 1: Simple Linear Regression

First wanted to create a simple linear regression model to:

1.  Simplicity
2.  Baseline Comparison
3.  Strong Predictive Power Expected

```{r Model 1}
#| echo: false
#| message: false
#| warning: false

# Target variable
y <- PL_Data$Final_Points

# Predictors
X <- PL_Data[, c("Differential", "Goals_Against", "Win_Percentage")]

# Combine X and y into one data frame
df_model <- data.frame(Final_Points = y, X)

# Split into training and testing sets (75% train, 25% test)
set.seed(42)
train_index <- createDataPartition(df_model$Final_Points, p = 0.75, list = FALSE)
train_data <- df_model[train_index, ]
test_data <- df_model[-train_index, ]

# Simple linear regression
simple_model <- lm(Final_Points ~ Differential, data = train_data)
summary(simple_model)
```

# M1 Findings:

```{r simple-model-table}
#| echo: false
#| message: false
#| warning: false

# Run the simple model
simple_model <- lm(Final_Points ~ Differential, data = train_data)

# Tidy the model output
tidy_simple_model <- tidy(simple_model)

# Display it nicely
kable(tidy_simple_model, digits = 3, caption = "Simple Linear Regression Results")


```

| Metric                  | Value      |
|:------------------------|:-----------|
| Residual Standard Error | 6.664      |
| Multiple R-squared      | 0.865      |
| Adjusted R-squared      | 0.863      |
| F-statistic             | 473.4      |
| p-value                 | \< 2.2e-16 |

# Model 2: Multiple Linear Regression

Next we wanted to use a multiple linear regression model to:

1.  Capture Multiple Influences
2.  Improve Prediction Accuracy
3.  Quantify Individual Contributions

# Model Summary:

| Metric                  | Value              |
|:------------------------|:-------------------|
| Residual Standard Error | 4.803              |
| Multiple R-squared      | 0.932              |
| Adjusted R-squared      | 0.929              |
| F-statistic             | 327.3 (df = 3, 72) |
| p-value                 | \< 2.2e-16         |
| R² Score (test set)     | 0.92               |
| RMSE (test set)         | 5.79               |

```{r}
#| echo: false
#| message: false
#| warning: false
# Fit linear regression model
model <- lm(Final_Points ~ Differential + Goals_Against + Win_Percentage, data = train_data)


summary(model)

# Predict on test data
predictions <- predict(model, newdata = test_data)

r2 <- R2(predictions, test_data$Final_Points)
rmse_val <- rmse(test_data$Final_Points, predictions)

cat("R² Score:", round(r2, 3), "\n")
cat("RMSE:", round(rmse_val, 2), "\n")
```

```{r}
#| echo: false
#| message: false
#| warning: false
# training control with 10-fold CV
train_control <- trainControl(method = "cv", number = 10)

# Cross-Validation
cv_model <- train(
  Final_Points ~ Differential + Goals_Against + Win_Percentage,
  data = df_model,
  method = "lm",
  trControl = train_control
)

print(cv_model)
```

# Model 3: Lasso Regression

# M3 Findings:

# Discuss Our Findings all together:

# Limitations:

# Future Works:

# 
