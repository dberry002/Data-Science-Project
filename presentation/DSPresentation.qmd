---
title: "Predicting Premier League Final Standings"
author: "By: Daniel and Aiden"
execute:
  echo: true
format:
  revealjs:
    theme: default 
editor: 
  markdown: 
    wrap: 72
engine: knitr
---

```{r packages}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(reticulate)
library(openintro)
library(readr)
library(ggcorrplot)
library(dplyr)
library(caret)      
library(Metrics)      
library(ggplot2)
library(broom)
library(knitr)

set.seed(1234)
```

# Research Question

**Can a team's performance metrics after the first half of the English
Premier League season reliably predict their final standings?**

::: center
![](StadiumImage.jpg){fig-align="center" width="275"}
:::

# Motivation

-   The EPL is one of the most competitive soccer leagues in the world.

-   Early-season performance often hints at final outcomes

-   Stronger predictions can help teams and analyst make strategic mid
    season decisions.

# Data Description

**Source:** worldfootball.net

**Collection:** Manual copy into excel, feature engineering, saved as
CSV.

**Scope:** 5 Premier League Seasons

**Outcome Variable:** Final_Points

**Explanatory Variables:** Goals_For, Goals_Against, Differential, Win
Percentage, and many more.

# Data Cleaning

1.  Removed irrelevant columns (e.g., logos).

2.  Parsed goal differential into separate Goals_For and Goals_Against
    columns.

3.  Created new variables: GPM, GAPM, Goals_By_Win.

4.  Categorized teams: Aggressive, Dominant, Defensive, and Under
    performers.

```{r}
#| echo: false
#| message: false
#| warning: false
# First for our slides it is important to import the data again and set it up the way it was in our analysis. Just made a copy of the data set to make it easier to load our models.
PL_Data <- read_csv("PL_data2.csv")

```

```{r Column Renaming}
#| echo: false
#| message: false
#| warning: false
# We want to make sure an audience that is not familiar with sports/soccer terms understand what are variables stand for. So renaming the columns is good practice to do this. 

PL_Data <- PL_Data %>%
  rename(Differential = Dif, Goals_For = GF, Goals_Against = GA, Goals_Per_Match = GPM, Goals_Against_Per_Match = GAPM, Attack_Defense_Ratio = ADR, Goals_Against_By_Wins = GCBW)

# Also hiding this code but talked about it in the presentation.
```

# Variable Correlations

These insights support our variable choices for regression models.

```{r Correlation Matrix, fig.width=10, fig.height=10, warning = FALSE}
#| echo: false
#| message: false
#| warning: false
correlation_matrix <- cor(PL_Data %>% select(
  Final_Points, Points, W, L, D, Goals_For, Goals_Against,
  Differential, Goals_Per_Match, Goals_Against_Per_Match,
  Attack_Defense_Ratio, Goals_Against_By_Wins, Goals_by_Win
))

ggcorrplot(
  correlation_matrix,
  lab = TRUE,
  type = "lower",
  lab_size = 3.5,
  tl.cex = 10,
  tl.srt = 45,           # Rotate text
  colors = c("blue", "white", "red"),
  ggtheme = theme_minimal()
) +
  labs(
    title = "Heatmap of Midseason Metrics and Final Points",
    subtitle = "Red: strong positive correlation; Blue: strong negative correlation"
  ) +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11),
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_text(size = 10)
  )

```

# Relationship Between Variables

-   As GAPM increases—meaning teams are conceding more goals per
    match—their final point totals tend to decrease.

    ```{r}
    #| echo: false
    #| fig-align: center
    knitr::include_graphics("Linear.png")
    ```

```{r}
#| echo: false
#| message: false
#| warning: false
ggplot(PL_Data, aes(x = Goals_Against_Per_Match, y = Final_Points)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", color = "red") +
  labs(title = "Points vs. Goals Against Per Match",
       x = "Goals Against Per Match (GPM)",
       y = "Points") +
  theme_minimal()
```

# Model 1: Simple Linear Regression

First wanted to create a simple linear regression model for:

1.  Simplicity
2.  Baseline Comparison
3.  Strong Predictive Power Expected

```{r Model 1}
#| echo: false
#| message: false
#| warning: false

# Target variable
y <- PL_Data$Final_Points

# Predictors
X <- PL_Data[, c("Differential", "Goals_Against", "Win_Percentage")]

# Combine X and y into one data frame
df_model <- data.frame(Final_Points = y, X)

# Split into training and testing sets (75% train, 25% test)
set.seed(42)
train_index <- createDataPartition(df_model$Final_Points, p = 0.75, list = FALSE)
train_data <- df_model[train_index, ]
test_data <- df_model[-train_index, ]

# Simple linear regression
simple_model <- lm(Final_Points ~ Differential, data = train_data)
summary(simple_model)
```

# M1 Findings:

```{r simple-model-table}
#| echo: false
#| message: false
#| warning: false

# Run the simple model
simple_model <- lm(Final_Points ~ Differential, data = train_data)

# Tidy the model output
tidy_simple_model <- tidy(simple_model)

# Display it nicely
kable(tidy_simple_model, digits = 3, caption = "Simple Linear Regression Results")


```

| Metric                  | Value      |
|:------------------------|:-----------|
| Residual Standard Error | 6.664      |
| Multiple R-squared      | 0.865      |
| p-value                 | \< 2.2e-16 |
| F-statistic             | 473.4      |
|                         |            |

# Model 2: Multiple Linear Regression

Next we wanted to use a multiple linear regression model to:

1.  Capture Multiple Influences
2.  Improve Prediction Accuracy
3.  Quantify Individual Contributions

# Model Summary:

| Metric                  | Value              |
|:------------------------|:-------------------|
| Residual Standard Error | 4.803              |
| Multiple R-squared      | 0.932              |
| Adjusted R-squared      | 0.929              |
| F-statistic             | 327.3 (df = 3, 72) |
| p-value                 | \< 2.2e-16         |
| R² Score (test set)     | 0.92               |
| RMSE (test set)         | 5.79               |

```{r}
#| echo: false
#| message: false
#| warning: false
# Fit linear regression model
model <- lm(Final_Points ~ Differential + Goals_Against + Win_Percentage, data = train_data)


summary(model)

# Predict on test data
predictions <- predict(model, newdata = test_data)

r2 <- R2(predictions, test_data$Final_Points)
rmse_val <- rmse(test_data$Final_Points, predictions)

cat("R² Score:", round(r2, 3), "\n")
cat("RMSE:", round(rmse_val, 2), "\n")
```

```{r}
#| echo: false
#| message: false
#| warning: false
# training control with 10-fold CV
train_control <- trainControl(method = "cv", number = 10)

# Cross-Validation
cv_model <- train(
  Final_Points ~ Differential + Goals_Against + Win_Percentage,
  data = df_model,
  method = "lm",
  trControl = train_control
)

print(cv_model)
```

# Model 3: Lasso Regression

-   Used for feature selection and reducing overfitting

-   Helps identify the most important predictors

-   Uses cross-validation with GroupKFold to prevent data leakage across
    seasons

Average Mean Squared Error (MSE): -18.48

-   This means predictions were off by about 4 points per season

# Feature Importance Plot

```{python}
#| echo: false
#| message: false
#| warning: false

# Loading packages in
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import make_column_transformer
from sklearn.model_selection import cross_validate, GroupKFold
from sklearn.linear_model import LinearRegression, RidgeCV, ElasticNetCV


# reading in csv
PL_data = pd.read_csv("PL_data2.csv")

# dropping nan and irrelevant cols
PL_data = PL_data.drop(columns=['Pos','#']) 
#defining y var
y = PL_data['Final_Points']

# identifying cat col
categorical_features = ['team_category'] 

# defining x var
X = PL_data.drop(columns=['Final_Points','Season','Team','Points'])  

#initializing GroupKFold
group_kfold = GroupKFold(n_splits=5)

# initializing preprocessor
preprocessor = make_column_transformer(
    (OneHotEncoder(), categorical_features),
    (StandardScaler(), X.select_dtypes(include=np.number).columns)
)

# initializing pipeline function 
def make_pipeline(estimator):
    return Pipeline([
        ('preprocessor', preprocessor),
        ('estimator', estimator)
    ])
    
# Creating the Lasso Pipeline
lasso_pipe = make_pipeline(ElasticNetCV(alphas=np.logspace(-3, 3, 100), l1_ratio=1, max_iter=20000))

# writing the function for cv
def cv_model(pipeline):
    cv_mod = cross_validate(
        estimator=pipeline,
        X=X,
        y=y,
        cv=group_kfold,
        groups = PL_data['Season'],
        scoring='neg_mean_squared_error',
        return_estimator=True,
        n_jobs=1
    )
    return cv_mod

cv_lasso = cv_model(lasso_pipe)

pd.set_option('display.max_columns', None) 

numeric_features = ['M', 'W', 'D', 'L', 'GF', 'GA', 'Dif', 'Goals_Per_Match', 'Goals_Against_Per_Match',
    'Win_Percentage', 'Loss_Percentage', 'Draw_Percentage', 
    'Goals_by_Win', 'GCBW', 'ADR']
                   
team_cat__features = ['team_category_Aggressive', 'team_category_Defensive', 'team_category_Dominant', 'team_category_Underperformers']

feature_names = team_cat__features + numeric_features


coef_df = pd.DataFrame(
    [model['estimator'].coef_ for model in cv_lasso['estimator']],
    columns=feature_names
)

plt.figure(figsize=(12, 6))
sns.stripplot(
    data=coef_df
)

#added a reference line
plt.axhline(0, color='red', linestyle='--', linewidth=1)
#added title and axis
plt.title('LASSO Coefs for each fold', fontsize=14)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Coefficient Value', fontsize=12)
_ = plt.xticks(rotation=90)
plt.tight_layout()

plt.savefig('lasso_coef_plot.png')
None 
```

# Top Predictors from Lasso:

1.  Wins (9.592635)

-   For every 1 standard deviation increase in Wins, final points
    increase by 9.592635.

2.  Losses (-6.489302)

3.  Attack-Defense Ratio (1.299359)

4.  Goals Against (-0.491382)

## Less Important Features:

-   Team categories (Aggressive, Defensive, etc.)

-   Raw counts of Goals Scored

-   Draw percentage (frequently zeroed out)

*Important Note: We ran this model twice. The first time we included
mid-season points, but in this iteration we excluded it due to the
variable's dominance. When it was included Wins was zeroed out as
mid-season points captured the same information as wins just in a better
format for predicting final wins.*

# M3 Findings:

-   **Feature Selection**: LASSO automatically zeroed out less important
    predictors, focusing on Wins, losses, and defensive metrics.
-   **Defensive Emphasis**: Goals Against and Attack-Defense Ratio were
    consistently important, confirming that defensive performance is
    crucial for final standings.
-   **GroupKFold Cross-Validation**: Using season-based grouping
    prevented data leakage across seasons, ensuring a more robust model
    evaluation.

# M3 Findings Cont.

-   **Performance**: With an average error of about 4 points per team
    per season, the LASSO model offers reliable predictions while
    maintaining interpretability.
-   **Variable Consistency**: The variables that maintained non-zero
    coefficients across all cross-validation folds are the most reliable
    predictors for future seasons.

# Discussion:

## Key Insights Across All Models:

-   **Strong Predictive Power**: Mid-season metrics explain over 90% of
    variance in final standings

-   **Defensive Performance Matters**: Goals Against is consistently
    important in all models

-   **Model Progression**:

    -   Simple Linear Model (R² = 0.865)

    -   Multiple Regression (R² = 0.936, RMSE = 5.10)

    -   Lasso Regression (MSE ≈ 18.48, \~4 points off per team)

## Practical Application:

Our multiple regression formula: **Final_Points = 37.39 + 0.31 × Diff –
0.31 × GA + 60.34 × Win_Percentage**

For a mid-table team with:

-   Goal Differential = 6

-   Goals Against = 27

-   Win Percentage = 0.526 (10/19 wins)

**Predicted points: 62.61** (Actual: 66, only 3.4 points off)

# Limitations:

-   **Sample Size**: Only 5 seasons limits statistical power
-   **Mid-season Transfers**: No accounting for January transfer window
    impacts
-   **External Factors**: Injuries, managerial changes not considered
-   **Competition Dynamics**: Different competitive landscapes each
    season
-   **Strength of Schedule**: Remaining fixture difficulty not
    incorporated
-   **Pandemic Impact**: COVID-19 disruptions in 2019-2020 and 2020-2021
    seasons

# Future Works:

-   **Additional Variables**:

    -   Expected Goals (xG) metrics

    -   Possession percentages

    -   Distance covered

    -   Player availability indices

-   **Expanded Data**:

    -   More seasons from Premier League

    -   Comparative analysis with other top European leagues

# Future Works Cont.

-   **Advanced Modeling**:

    -   Time-series forecasting for in-season updates

    -   Position-specific models (Top 4, Mid-table, Relegation zone)

    -   Machine learning approaches for non-linear relationships

-   **Practical Applications**:

    -   Interactive prediction tools for fans and analysts

    -   Integration with betting market data
